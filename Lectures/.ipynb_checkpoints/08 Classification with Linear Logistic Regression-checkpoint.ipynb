{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\wv_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Linear Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a linear model used for classification can result in masking. We discussed fixing this by using different\n",
    "shaped membership functions, other than linear.\n",
    "\n",
    "Our first approach to this was to use generative models (Normal distributions) to model the data\n",
    "from each class, forming $p(\\xv|C=k)$.  Using Bayes Theorem, we converted this to $p(C=k|\\xv)$ and\n",
    "derived QDA and LDA.\n",
    "\n",
    "Now we will derive a linear model that directly predicts $p(C=k|\\xv)$, resulting in the algorithm called logisitic\n",
    "regression.  It is derived to maximize the likelihood of the data, given a bunch of samples and their class labels.\n",
    "\n",
    "Remember this picture?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs545/notebooks/figures/indicatorvarsmax2.png\" width=400>\n",
    "\n",
    "The problem was that the green line for Class 2 was too low.\n",
    "In fact, all lines are too low in the middle of x range.  Maybe we\n",
    "can reduce the masking effect by\n",
    "\n",
    "   * requiring the function values to be between 0 and 1, and\n",
    "   * requiring them to sum to 1 for every value of x.\n",
    "\n",
    "We can satisfy those two requirements by directly representing\n",
    "$p(C=k|\\xv)$ as\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      p(C=k|\\xv) = \\frac{f(\\xv;\\wv_k)}{\\sum_{m=1}^K f(\\xv;\\wv_m)}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "with $f(\\xv;\\wv) \\ge 0$. We haven't discussed the form of $f$ yet, but $\\wv$\n",
    "represents the parameters of $f$ that we will tune to fit the\n",
    "training data (later).\n",
    "\n",
    "This is certainly an expression that is between 0 and 1 for\n",
    "any $\\xv$.\n",
    "And we have $p(C=k|\\xv)$ expressed directly, as opposed to\n",
    "the previous generative approach of first modeling $p(\\xv|C=k)$\n",
    "and using Bayes' theorem to get $p(C=k|\\xv)$.\n",
    "\n",
    "Let's give the above expression another name\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      g_k(\\xv) = p(C=k|\\xv) = \\frac{f(\\xv;\\wv_k)}{\\sum_{m=1}^K f(\\xv;\\wv_m)}\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatever we choose for $f$, we must make a plan for\n",
    "optimizing its parameters $\\wv$.  How?\n",
    "\n",
    "Let's maximize the likelihood of the data.  So, what is the\n",
    "likelihood of training data consisting of samples $\\{\\xv_1, \\xv_2, \\ldots, \\xv_N\\}$ and class indicator variables\n",
    "\n",
    "$$\n",
    "  \\begin{align*}\n",
    "    \\begin{pmatrix}\n",
    "      t_{1,1} & t_{1,2} & \\ldots & t_{1,K}\\\\\n",
    "      t_{2,1} & t_{2,2} & \\ldots & t_{2,K}\\\\\n",
    "      \\vdots\\\\\n",
    "      t_{N,1} & t_{N,2} & \\ldots & t_{N,K}\n",
    "    \\end{pmatrix}\n",
    "  \\end{align*}\n",
    "$$\n",
    "\n",
    "with every value $t_{n,k}$ being 0 or 1, and each row of this matrix\n",
    "contains a single 1? (We can also express $\\{\\xv_1, \\xv_2,\n",
    "\\ldots, \\xv_N\\}$ as an $N \\times D$ matrix, but we will be using\n",
    "single samples $\\xv_n$ more often in the following.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is just the product of all $p(C=\\text{class of }\n",
    "n^\\text{th}\\text{ sample}\\,|\\,\\xv_n)$ values\n",
    "for sample $n$.  A common way to express this product, using those handy indicator variables is\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      L(\\betav) = \\prod_{n=1}^N \\prod_{k=1}^K p(C=k\\,|\\, \\xv_n)^{t_{n,k}}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Say we have three classes ($K=3$) and training sample $n$ is from Class 2, then the  product is\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        p(C=1\\,|\\,\\xv_n)^{t_{n,1}} p(C=2\\,|\\,\\xv_n)^{t_{n,2}}\n",
    "        p(C=3\\,|\\,\\xv_n)^{t_{n,3}} & = \n",
    "         p(C=1\\,|\\,\\xv_n)^0 p(C=2\\,|\\,\\xv_n)^1 p(C=3\\,|\\,\\xv_n)^0 \\\\\n",
    "        & = 1\\; p(C=2\\,|\\,\\xv_n)^1 \\; 1 \\\\\n",
    "        & = p(C=2\\,|\\,\\xv_n) \n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "This shows how the indicator variables as exponents select the correct terms to be included in the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing the Data Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to find $\\wv$ that maximizes the data likelihood.  How shall we proceed?\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      L(\\wv) & = \\prod_{n=1}^N \\prod_{k=1}^K p(C=k\\,|\\, \\xv_n) ^ {t_{n,k}}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Right.  Find the derivative with respect to each component of $\\wv$, or the gradient with respect to $\\wv$.  But there is\n",
    "a mess of products in this. So...\n",
    "\n",
    "Right again.  Work with the natural logarithm  $\\log L(\\wv)$ which we will call $LL(\\wv)$.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      LL(\\wv) = \\log L(\\wv) = \\sum_{n=1}^N \\sum_{k=1}^K t_{n,k}  \\log p(C=k\\,|\\,\\xv_n)\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the gradient of $LL(\\wv)$ with respect to\n",
    "$\\wv$ is not linear in $\\wv$, so we cannot simply set the\n",
    "result equal to zero and solve for $\\wv$.\n",
    "\n",
    "Instead, we do gradient ascent. (Why \"ascent\"?)\n",
    "\n",
    "  * Initialize $\\wv$ to some value.\n",
    "  * Make small change to $\\wv$ in the direction of the  gradient of $LL(\\wv)$ with respect to $\\wv$  (or $\\grad_{\\wv} LL(\\wv)$)\n",
    "  * Repeat above step until $LL(\\wv)$ seems to be at a maximum.\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\wv \\leftarrow \\wv + \\alpha \\grad_{\\wv} LL(\\wv)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a constant that affects the step size.\n",
    "\n",
    "Remember that $\\wv$ is a matrix of parameters, with, let's\n",
    "say, columns corresponding to the values required for each $f$, of\n",
    "which there are $K-1$.\n",
    "\n",
    "We can work on the update formula and $\\grad_{\\wv} LL(\\wv)$ one column at\n",
    "a time\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\wv_k  \\leftarrow \\wv_k + \\alpha \\grad_{\\wv_k} LL(\\wv)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "and combine them at the end.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\wv  \\leftarrow \\wv + \\alpha (\\grad_{\\wv_1} LL(\\wv),\n",
    "        \\grad_{\\wv_2} LL(\\wv), \\ldots, \\grad_{\\wv_{K-1}} LL(\\wv))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Remembering that $\\frac{\\partial \\log h(x)}{\\partial x} = \\frac{1}{h(x)}\\frac{\\partial h(x)}{x}$ and\n",
    "that $p(C=k|\\xv_n) = g_k(\\xv_n)$ \n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "      LL(\\wv) & = \\sum_{n=1}^N \\sum_{k=1}^K  t_{n,k} \\log p(C=k\\,|\\,\\xv_n)\\\\\n",
    "      & = \\sum_{n=1}^N \\sum_{k=1}^K t_{n,k} \\log g_k(\\xv_n)\\\\\n",
    "      \\grad_{\\wv_j} LL(\\wv) & = \\sum_{n=1}^N \\sum_{k=1}^K\n",
    "      \\frac{t_{n,k}}{g_k(\\xv_n)} \\grad_{\\wv_j} g_k(\\xv_n)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    " We also want $g_k(\\xv_n)$ to be the probability $P(C=k | \\xv_n)$, so need it to be between 0 and 1 for all $k$ and we want $\\sum_{k=1}^K g_k(\\xv_n) = 1$.  And, hey, wouldn't it be super nice if $\\grad_{\\wv_j} g_k(\\xv_n)$\n",
    "includes the factor $g_k(\\xv_n)$ so that it would cancel\n",
    "with the $g_k(\\xv_n)$ in the denominator. \n",
    "\n",
    "The following definition does all of this.  This is often referred to as a *softmax* function.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      g_k(\\xv_n) & = \\frac{\\ebx{k}}{\\sum_{m=1}^{K} \\ebx{m}}\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can work on $\\grad_{\\wv_j} g_k(\\xv_n)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_k(\\xv_n) = \\frac{\\ebx{k}}{\\sum_{m=1}^{K} \\ebx{m}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\grad_{\\wv_j} g_k(\\xv_n) & = \\grad_{\\wv_j} \\left (\\frac{\\ebx{k}}{\\sum_{m=1}^{K} \\ebx{m}} \\right )\\\\\n",
    "    & = \\grad_{\\wv_j} \\left [ \\left (\\sum_{m=1}^{K} \\ebx{m} \\right )^{-1} \\ebx{k} \\right ] \n",
    "    \\end{align*}\n",
    "$$\n",
    "Since\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\grad_{\\wv_j} \\ebx{k} &= \\begin{cases}\n",
    "\\xv_n \\ebx{k}, & \\text{if } k=j\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\grad_{\\wv_j} \\sum_{m=1}^K-1 \\ebx{m} &= \\xv_n \\ebx{k}\n",
    "\\end{align*}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\grad_{\\wv_j} g_k(\\xv_n) & = \\grad_{\\wv_j} \\left (\\frac{\\ebx{k}}{\\sum_{m=1}^{K} \\ebx{m}} \\right )\\\\\n",
    "    & = -1 \\left (\\sum_{m=1}^{K} \\ebx{m} \\right )^{-2} \\xv_n \\ebx{j}\n",
    "    \\ebx{k} + \\left (\\sum_{m=1}^{K} \\ebx{m} \\right )^{-1} \n",
    "    \\begin{cases} \\xv_n \\ebx{k},& \\text{if} j=k\\\\ 0,& \\text{otherwise} \\end{cases}\\\\\n",
    "& = -\\frac{\\ebx{k}}{\\sum_{m=1}^{K} \\ebx{m}}\n",
    "  \\frac{\\ebx{j}}{\\sum_{m=1}^{K} \\ebx{j}} \\xv_n +\n",
    "  \\begin{cases} \\frac{\\ebx{j}}{\\sum_{m=1}^{K} \\ebx{j}} \\xv_n,& \\text{if} j=k\\\\ 0,& \\text{otherwise} \\end{cases}\\\\\n",
    "%& = \\frac{\\ebx{k}}{\\sum_{m=1}^{K} \\ebx{m} } \n",
    "& = - g_k(\\xv_n) g_j(\\xv_n) \\xv_n + \\begin{cases} g_j(\\xv_n) \\xv_n,^ \\text{if} j=k\\\\ 0,& \\text{otherwise} \\end{cases}\\\\\n",
    "& = g_k(\\xv_n) (\\delta_{jk} - g_j(\\xv_n)) \\xv_n\n",
    "    \\end{align*}\n",
    "$$\n",
    "where $\\delta_{jk} = 1$ if $j=k$, 0 otherwise.\n",
    "\n",
    "Substituting this back into the log likelihood expression, we get\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\grad_{\\wv_j} LL(\\wv) & = \\sum_{n=1}^N \\sum_{k=1}^K \\frac{t_{n,k}}{g_k(\\xv_n)} \\grad_{\\wv_j} g_k(\\xv_n)\\\\\n",
    "    & = \\sum_{n=1}^N \\sum_{k=1}^K \\frac{t_{n,k}}{g_k(\\xv_n)} \\left (g_k(\\xv_n) (\\delta_{jk} - g_j(\\xv_n)) \\xv_n \\right )\\\\\n",
    "    & = \\sum_{n=1}^N \\left ( \\sum_{k=1}^K t_{n,k} \\delta_{jk} -\n",
    "  g_j(\\xv_n) \\sum_{k=1}^K t_{n,k} \\right ) \\xv_n\\\\\n",
    "& = \\sum_{n=1}^N  (t_{n,j} - g_j(\\xv_n)) \\xv_n\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "which results in this update rule for $\\wv_j$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\wv_j  \\leftarrow \\wv_j + \\alpha \\sum_{n=1}^N\n",
    "        (t_{n,j} - g_j(\\xv_n)) \\xv_n\n",
    "        \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "How do we do this in python?  First, a summary of the derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(C=k\\,|\\,\\xv_n)$ and the data likelihood we want to maximize:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      g_k(\\xv_n) & = P(C=k\\,|\\,\\xv_n) =\n",
    "      \\frac{f(\\xv_n;\\wv_k)}{\\sum_{m=1}^{K} f(\\xv_n;\\wv_m)}\\\\\n",
    "      f(\\xv_n;\\wv_k) & = \\left \\{ \\begin{array}{ll} \\ebx{k}; & k < K\\\\ 1;& k = K \\end{array} \\right .\\\\\n",
    "      L(\\wv) & = \\prod_{n=1}^N \\prod_{k=1}^K p(C=k\\,|\\, \\xv_n) ^{t_{n,k}}\\\\\n",
    "      & = \\prod_{n=1}^N \\prod_{k=1}^K g_k(\\xv_n)^{t_{n,k}}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Gradient of log likelihood with respect to $\\wv_j$:\n",
    "\n",
    "$$         \n",
    "    \\begin{align*}\n",
    "      \\grad_{\\wv_j} LL(\\wv) & = \\sum_{n=1}^N \\sum_{k=1}^K\n",
    "      \\frac{t_{n,k}}{g_k(\\xv_n)} \\grad_{\\wv_j}\n",
    "      g_k(\\xv_n)\\\\\n",
    "%& = \\sum_{n=1}^N \\left ( \\sum_{k=1}^K t_{n,k} \\delta_{jk} -\n",
    "%  g_j(\\xv_n) \\sum_{k=1}^K t_{n,k} \\right )\\\\\n",
    "& = \\sum_{n=1}^N \\xv_n (t_{n,j} - g_j(\\xv_n))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which results in this update rule for $\\wv_j$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\wv_j  \\leftarrow \\wv_j + \\alpha \\sum_{n=1}^N\n",
    "        (t_{n,j} - g_j(\\xv_n)) \\xv_n\n",
    "        \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update rule for $\\wv_j$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\wv_j  \\leftarrow \\wv_j + \\alpha \\sum_{n=1}^N\n",
    "      (t_{n,j} - g_j(\\xv_n)) \\xv_n\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "What are shapes of each piece?  Remember that whenever we are dealing with weighted sums of inputs, as we are here, add the constant 1 to the front of each sample.\n",
    "\n",
    "  * $\\xv_n$ is $(D+1) \\times 1$ ($+1$ for the constant 1 input)\n",
    "  * $\\wv_j$ is  $(D+1) \\times 1$ \n",
    "  * $t_{n,j} - g_j(\\xv_n)$ is   a scalar\n",
    "\n",
    "So, this all works. But, notice the sum is over $n$, and each\n",
    "term in the product as $n$ components, so we can do this as a dot product.\n",
    "\n",
    "Let's remove the sum and replace subscript $n$ with\n",
    "*. \n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\wv_j  &\\leftarrow \\wv_j + \\alpha \\sum_{n=1}^N\n",
    "      (t_{n,j} - g_j(\\xv_n)) \\xv_n\\\\\n",
    "      \\wv_j  &\\leftarrow \\wv_j + \\alpha (t_{*,j} - g_j(\\xv_*)) \\xv_*\\\\\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "What are shapes of each piece?\n",
    "\n",
    "  * $(t_{*,j} - g_j(\\xv_*))$ is $N \\times 1$\n",
    "  * $\\xv_* = X$ is  $N \\times (D+1)$\n",
    "  * $\\wv_j$ is  $(D+1) \\times 1$ \n",
    "\n",
    "So, this will work if we transpose $X$ and premultiply it and define\n",
    "$g$ as a function that accepts $\\Xv$.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "%      \\wv_j  &\\leftarrow \\wv_j + \\alpha (t_{*,j} -\n",
    "%      g(\\xv_*;\\wv_j)) \\xv_*\\\\\n",
    "      \\wv_j  &\\leftarrow \\wv_j + \\alpha \\Xv^T (t_{*,j} -\n",
    "      g_j(\\Xv))\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep going...and try to make this expression work for\n",
    "all of the $\\wv$'s.\n",
    "Playing with the subscripts again, replace $j$ with *.\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\wv_j  &\\leftarrow \\wv_j + \\alpha \\Xv^T (t_{*,j} - g_j(\\Xv))\\\\\n",
    "      \\wv_*  &\\leftarrow \\wv_* + \\alpha \\Xv^T (t_{*,*} - g_*(\\Xv))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Now what are shapes? \n",
    "\n",
    "  * $\\wv_* = \\wv$ is  $(D+1) \\times K$\n",
    "  * $t_{*,*} = T$ is  $N \\times K$\n",
    "  * $g_*(\\Xv)$ is   $N \\times (K-1)$\n",
    "  * $t_{*,*} - g_*(\\Xv)$ is  $N \\times K$\n",
    "  * So, $\\Xv^T (t_{*,*} - g_*(\\Xv))$ is  $(D+1) \\times K$\n",
    "  * So, $\\Xv^T (T - g(\\Xv))$ is  $(D+1) \\times K$\n",
    "\n",
    "Now our update equation for all $\\wv$'s is\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      \\wv  &\\leftarrow \\wv + \\alpha \\Xv^T (T - g(\\Xv))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "We had defined, for $k = 1,\\ldots, K$,\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        g_k(\\xv) &=  \\dfrac{\\ebx{k}}{\\sum_{m=1}^K \\ebx{m}}\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "Changing these to handle all samples $\\Xv$ and all parameters\n",
    "$\\wv$ we have\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "     g(\\Xv) & = \\frac{e^{\\Xv \\wv}}{\\text{rowSums}(e^{\\Xv \\wv})}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Given training data $\\Xv$ ($N\\times (D+1)$) and class\n",
    "indicator variables $T$ ($N \\times K)$), these expressions\n",
    "can be performed with the following code.\n",
    "\n",
    "First, we need a function to create indicator variables from the class labels, to get\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "2\\\\\n",
    "2\\\\\n",
    "1\\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:35.781565Z",
     "start_time": "2022-09-29T00:05:35.770638Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.268674Z",
     "start_time": "2022-09-29T00:05:35.782984Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.278294Z",
     "start_time": "2022-09-29T00:05:36.269908Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_indicator_vars(T):\n",
    "    # Make sure T is two-dimensional. Should be nSamples x 1.\n",
    "    if T.ndim == 1:\n",
    "        T = T.reshape((-1, 1))    \n",
    "    return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.292352Z",
     "start_time": "2022-09-29T00:05:36.280485Z"
    }
   },
   "outputs": [],
   "source": [
    "T = np.array([1,2,2,1,3]).reshape((-1,1))\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.301577Z",
     "start_time": "2022-09-29T00:05:36.293433Z"
    }
   },
   "outputs": [],
   "source": [
    "make_indicator_vars(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.310199Z",
     "start_time": "2022-09-29T00:05:36.302716Z"
    }
   },
   "outputs": [],
   "source": [
    "def g(X, w):\n",
    "    fs = np.exp(X @ w)  # N x K\n",
    "    denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
    "    gs = fs / denom\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```g``` is sometimes called the *softmax* function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.319588Z",
     "start_time": "2022-09-29T00:05:36.311800Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X, w):\n",
    "    fs = np.exp(X @ w)  # N x K\n",
    "    denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
    "    gs = fs / denom\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the updates to $\\wv$ can be formed with code like this.\n",
    "\n",
    "    TI = make_indicator_vars(T)   \n",
    "    w = np.zeros((X.shape[1], TI.shape[1]))\n",
    "    alpha = 0.0001\n",
    "    for step in range(1000):\n",
    "        Y = softmax(X, w)\n",
    "        w = w + alpha * X.T @ (TI - Y)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code for applying linear logistic regression to the Parkinsons data from last lecture:  [parkinsons data set](https://archive.ics.uci.edu/ml/datasets/Parkinsons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.498877Z",
     "start_time": "2022-09-29T00:05:36.320640Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('parkinsons.data')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.511063Z",
     "start_time": "2022-09-29T00:05:36.500105Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data\n",
    "X = X.drop(['status', 'name'], axis=1)\n",
    "Xnames = X.columns.tolist()\n",
    "X = X.values\n",
    "\n",
    "T = data['status'].values\n",
    "T = T.reshape((-1, 1))\n",
    "Tname = 'status'\n",
    "\n",
    "X.shape, Xnames, T.shape, Tname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.522130Z",
     "start_time": "2022-09-29T00:05:36.513597Z"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(X, mean, stds):\n",
    "    return (X - mean) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.533221Z",
     "start_time": "2022-09-29T00:05:36.523799Z"
    }
   },
   "outputs": [],
   "source": [
    "import qdalda   # from previous lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate our training, validation and testing partitions we need to partition data into folds on a class by class basis to make each fold have appoximately the same proportion of samples from each class.  Here is a function that does this. This form of partitioning is referred to as \"stratified\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.549931Z",
     "start_time": "2022-09-29T00:05:36.534944Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_stratified_partitions(X, T, n_folds, validation=True, shuffle=True):\n",
    "    '''Generates sets of  Xtrain,Ttrain,Xvalidate,Tvalidate,Xtest,Ttest\n",
    "      or\n",
    "       sets of Xtrain,Ttrain,Xtest,Ttest if validation is False\n",
    "    Build dictionary keyed by class label. Each entry contains rowIndices and start and stop\n",
    "    indices into rowIndices for each of n_folds folds'''\n",
    "\n",
    "    def rows_in_fold(folds, k):\n",
    "        all_rows = []\n",
    "        for c, rows in folds.items():\n",
    "            class_rows, starts, stops = rows\n",
    "            all_rows += class_rows[starts[k]:stops[k]].tolist()\n",
    "        return all_rows\n",
    "\n",
    "    def rows_in_folds(folds, ks):\n",
    "        all_rows = []\n",
    "        for k in ks:\n",
    "            all_rows += rows_in_fold(folds, k)\n",
    "        return all_rows\n",
    "\n",
    "    row_indices = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(row_indices)\n",
    "    folds = {}\n",
    "    classes = np.unique(T)\n",
    "    for c in classes:\n",
    "        class_indices = row_indices[np.where(T[row_indices, :] == c)[0]]\n",
    "        n_in_class = len(class_indices)\n",
    "        n_each = int(n_in_class / n_folds)\n",
    "        starts = np.arange(0, n_each * n_folds, n_each)\n",
    "        stops = starts + n_each\n",
    "        stops[-1] = n_in_class\n",
    "        folds[c] = [class_indices, starts, stops]\n",
    "\n",
    "    for test_fold in range(n_folds):\n",
    "        if validation:\n",
    "            for validate_fold in range(n_folds):\n",
    "                if test_fold == validate_fold:\n",
    "                    continue\n",
    "                train_folds = np.setdiff1d(range(n_folds), [test_fold, validate_fold])\n",
    "                rows = rows_in_fold(folds, test_fold)\n",
    "                Xtest = X[rows, :]\n",
    "                Ttest = T[rows, :]\n",
    "                rows = rows_in_fold(folds, validate_fold)\n",
    "                Xvalidate = X[rows, :]\n",
    "                Tvalidate = T[rows, :]\n",
    "                rows = rows_in_folds(folds, train_folds)\n",
    "                Xtrain = X[rows, :]\n",
    "                Ttrain = T[rows, :]\n",
    "                yield Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest\n",
    "        else:\n",
    "            # No validation set\n",
    "            train_folds = np.setdiff1d(range(n_folds), [test_fold])\n",
    "            rows = rows_in_fold(folds, test_fold)\n",
    "            Xtest = X[rows, :]\n",
    "            Ttest = T[rows, :]\n",
    "            rows = rows_in_folds(folds, train_folds)\n",
    "            Xtrain = X[rows, :]\n",
    "            Ttrain = T[rows, :]\n",
    "            yield Xtrain, Ttrain, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.561691Z",
     "start_time": "2022-09-29T00:05:36.551106Z"
    }
   },
   "outputs": [],
   "source": [
    "for Xtrain, Ttrain, Xval, Tval, Xtest, Ttest in generate_stratified_partitions(X, T, 4):\n",
    "    print(f'{len(Ttrain)} {np.mean(Ttrain == 0):.3f} {len(Tval)} {np.mean(Tval == 0):.3f} {len(Ttest)} {np.mean(Ttest == 0):.3f}')\n",
    "print('\\n', np.mean(T == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function that can iterate over all ways of making training, validation and test sets from n_folds partitions and train QDA, LDA and linear logistic regression models to the data.\n",
    "\n",
    "In the following function the `break` statement at the end of the for loop stops execution after just one run of the cross-validation method, to save time.  To use all runs, we would have to calculate the mean errors, or accuracies, over all cross-validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:36.577456Z",
     "start_time": "2022-09-29T00:05:36.562898Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_park_logreg(X, T, n_folds):\n",
    "\n",
    "    for Xtrain, Ttrain, Xtest, Ttest in generate_stratified_partitions(X, T, n_folds, validation=False):\n",
    "\n",
    "        means,stds = np.mean(Xtrain, 0), np.std(Xtrain ,0)\n",
    "        Xtrains = standardize(Xtrain, means, stds)\n",
    "        Xtests = standardize(Xtest, means, stds)\n",
    "\n",
    "        Xtrains1 = np.hstack(( np.ones((Xtrains.shape[0], 1)), Xtrains))\n",
    "        Xtests1 = np.hstack(( np.ones((Xtests.shape[0], 1)), Xtests))\n",
    "\n",
    "        # New stuff for linear logistic regression\n",
    "\n",
    "        TtrainI = make_indicator_vars(Ttrain)\n",
    "        TtestI = make_indicator_vars(Ttest)\n",
    "\n",
    "        w = np.zeros((Xtrains1.shape[1], TtrainI.shape[1]))\n",
    "        likelihood = []\n",
    "        alpha = 0.0001\n",
    "        for step in range(10000):\n",
    "            # forward pass\n",
    "            gs = softmax(Xtrains1, w)\n",
    "            # backward pass and weight update\n",
    "            w = w + alpha * Xtrains1.T @ (TtrainI - gs)\n",
    "            # convert log likelihood to likelihood\n",
    "            likelihoodPerSample = np.exp( np.sum(TtrainI * np.log(gs)) / Xtrains.shape[0])\n",
    "            likelihood.append(likelihoodPerSample)\n",
    "\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        \n",
    "        plt.subplot2grid((1, 4), (0, 0))\n",
    "        plt.plot(likelihood)\n",
    "        plt.ylabel('Likelihood')\n",
    "        plt.xlabel('Epoch')\n",
    "\n",
    "        logregOutput = g(Xtrains1, w)\n",
    "        predictedTrain = np.argmax(logregOutput, axis=1)\n",
    "        logregOutput = g(Xtests1, w)\n",
    "        predictedTestLR = np.argmax(logregOutput, axis=1)\n",
    "\n",
    "        print(\"LogReg: Percent correct: Train {:.3g} Test {:.3g}\".format(percent_correct(predictedTrain, Ttrain),\n",
    "                                                                         percent_correct(predictedTestLR, Ttest)))\n",
    "\n",
    "        # Previous QDA, LDA code\n",
    "\n",
    "        qda = qdalda.QDA()\n",
    "        qda.train(Xtrain, Ttrain)\n",
    "        qdaPredictedTrain = qda.use(Xtrain)\n",
    "        qdaPredictedTest = qda.use(Xtest)\n",
    "        print(\"   QDA: Percent correct: Train {:.3g} Test {:.3g}\".format(percent_correct(qdaPredictedTrain, Ttrain),\n",
    "                                                                         percent_correct(qdaPredictedTest, Ttest)))\n",
    "\n",
    "        lda = qdalda.LDA()\n",
    "        lda.train(Xtrain, Ttrain)\n",
    "        ldaPredictedTrain = qda.use(Xtrain)\n",
    "        ldaPredictedTest = qda.use(Xtest)\n",
    "        print(\"   LDA: Percent correct: Train {:.3g} Test {:.3g}\".format(percent_correct(ldaPredictedTrain, Ttrain),\n",
    "                                                                         percent_correct(ldaPredictedTest, Ttest)))\n",
    "\n",
    "        plt.subplot2grid((1, 4), (0, 1), colspan=3)\n",
    "        plt.plot(Ttest, 'o-', label='Target')\n",
    "        plt.plot(predictedTestLR, 'o-', label='LR')\n",
    "        plt.plot(qdaPredictedTest, 'o-', label='QDA')\n",
    "        plt.plot(ldaPredictedTest, 'o-', label='LDA')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Class')\n",
    "        plt.xlabel('Sample')\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        break  # only do one data partition\n",
    "\n",
    "def percent_correct(p, t):\n",
    "    return np.sum(p.ravel()==t.ravel()) / float(len(t)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:37.028152Z",
     "start_time": "2022-09-29T00:05:36.578750Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_park_logreg(X, T, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:37.552163Z",
     "start_time": "2022-09-29T00:05:37.029574Z"
    }
   },
   "outputs": [],
   "source": [
    "run_park_logreg(X, T, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:06:12.126107Z",
     "start_time": "2022-09-29T00:06:11.689441Z"
    }
   },
   "outputs": [],
   "source": [
    "run_park_logreg(X, T, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is doing stochastic gradient descent (SGD) in the gradient of the log\n",
    "likelihood.  Do we have a better way of doing this gradient ascent?  Sure, **let's try Adam**.  But first we must define our error function to be minimized and its gradient function.\n",
    "\n",
    "The\n",
    "function to be optimized should be the negative of the log likelihood,\n",
    "because SGD is designed to minimize the function.  And the gradient\n",
    "function must also include this negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Here are definitions of the log likehood and its gradient again.\n",
    "$$\n",
    "      \\begin{align*}\n",
    "      LL(\\wv) & = \\sum_{n=1}^N \\sum_{k=1}^K t_{n,k} \\log g_k(\\xv_n)\\\\\n",
    "      \\grad_{\\wv_j} LL(\\wv)  & = \\sum_{n=1}^N \\xv_n (t_{n,j} - g_j(\\xv_n))\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "or, as matrices, and using the mean log likelihood,\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "    Y &= g(\\Xv)\\\\\n",
    "    LL(\\wv) & = \\text{np.mean}(T \\cdot \\log Y , \\text{axis}=0) \\\\\n",
    "      \\grad_{\\wv_j} LL(\\wv) & =  \\Xv^T (T - Y) \\;/\\; (N\\,K)\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:38.180904Z",
     "start_time": "2022-09-29T00:05:38.152933Z"
    }
   },
   "outputs": [],
   "source": [
    "import optimizers\n",
    "\n",
    "\n",
    "def run_park_logreg2(X, T, n_folds):\n",
    "\n",
    "    for Xtrain, Ttrain, Xtest, Ttest in generate_stratified_partitions(X, T, n_folds, validation=False):\n",
    "\n",
    "        means,stds = np.mean(Xtrain,0), np.std(Xtrain,0)\n",
    "        Xtrains = standardize(Xtrain,means,stds)\n",
    "        Xtests = standardize(Xtest,means,stds)\n",
    "\n",
    "        Xtrains1 = np.hstack(( np.ones((Xtrains.shape[0],1)), Xtrains))\n",
    "        Xtests1 = np.hstack(( np.ones((Xtests.shape[0],1)), Xtests))\n",
    "\n",
    "        TtrainI = make_indicator_vars(Ttrain)\n",
    "        TtestI = make_indicator_vars(Ttest)\n",
    "\n",
    "        n_classes = TtrainI.shape[1]\n",
    "\n",
    "        all_weights = np.zeros(Xtrains1.shape[1] * TtrainI.shape[1])\n",
    "        \n",
    "        w = all_weights.reshape(( Xtrains1.shape[1], TtrainI.shape[1])) # n_inputs x n_classes\n",
    "\n",
    "        def softmax(X):\n",
    "            fs = np.exp(X @ w)  # N x K\n",
    "            denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
    "            gs = fs / denom\n",
    "            return gs\n",
    "\n",
    "        def neg_log_likelihood():\n",
    "            # w = warg.reshape((-1,K))\n",
    "            Y = softmax(Xtrains1)\n",
    "            return - np.mean(TtrainI * np.log(Y))\n",
    "\n",
    "        def gradient_neg_log_likelihood():\n",
    "            Y = softmax(Xtrains1)\n",
    "            grad = Xtrains1.T @ (Y - TtrainI) / (TtrainI.shape[0] * TtrainI.shape[1])\n",
    "            return grad.reshape((-1))\n",
    "\n",
    "\n",
    "        optimizer = optimizers.Optimizers(all_weights)\n",
    "        to_likelihood = lambda nll: np.exp(-nll)\n",
    "        \n",
    "        likelihood_trace = optimizer.adam(neg_log_likelihood, gradient_neg_log_likelihood,\n",
    "                                         n_epochs=10000, learning_rate=0.01, error_convert_f=to_likelihood)\n",
    "\n",
    "\n",
    "        logregOutput = softmax(Xtrains1)\n",
    "        predictedTrain = np.argmax(logregOutput,axis=1)\n",
    "        logregOutput = softmax(Xtests1)\n",
    "        predictedTest = np.argmax(logregOutput,axis=1)\n",
    "\n",
    "        print(\"LogReg: Percent correct: Train {:.3g} Test {:.3g}\".format(percent_correct(predictedTrain,Ttrain),percent_correct(predictedTest,Ttest)))\n",
    "\n",
    "        plt.plot(likelihood_trace)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Likelihood')\n",
    "        \n",
    "        # Previous QDA code\n",
    "\n",
    "        qda = qdalda.QDA()\n",
    "        qda.train(Xtrain, Ttrain)\n",
    "        qdaPredictedTrain = qda.use(Xtrain)\n",
    "        qdaPredictedTest = qda.use(Xtest)\n",
    "        print(\"   QDA: Percent correct: Train {:.3g} Test {:.3g}\".format(percent_correct(qdaPredictedTrain, Ttrain),\n",
    "                                                                         percent_correct(qdaPredictedTest, Ttest)))\n",
    "\n",
    "        lda = qdalda.LDA()\n",
    "        lda.train(Xtrain, Ttrain)\n",
    "        ldaPredictedTrain = qda.use(Xtrain)\n",
    "        ldaPredictedTest = qda.use(Xtest)\n",
    "        print(\"   LDA: Percent correct: Train {:.3g} Test {:.3g}\".format(percent_correct(ldaPredictedTrain, Ttrain),\n",
    "                                                                         percent_correct(ldaPredictedTest, Ttest)))\n",
    "        \n",
    "        break # remove to show all partitioning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:39.140134Z",
     "start_time": "2022-09-29T00:05:38.182532Z"
    }
   },
   "outputs": [],
   "source": [
    "run_park_logreg2(X, T, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-29T00:05:40.031936Z",
     "start_time": "2022-09-29T00:05:39.141738Z"
    }
   },
   "outputs": [],
   "source": [
    "run_park_logreg2(X, T, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you change this to run SGD instead of Adam for the linear logistic regression?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
