{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: NeuralNetwork Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-1\">Requirements</a></span></li><li><span><a href=\"#Code-for-NeuralNetwork-Class-Saved-in-File-neuralnetworkA3.py\" data-toc-modified-id=\"Code-for-NeuralNetwork-Class-Saved-in-File-neuralnetworkA3.py-2\">Code for <code>NeuralNetwork</code> Class Saved in File <code>neuralnetworkA3.py</code></a></span></li><li><span><a href=\"#Example-Results\" data-toc-modified-id=\"Example-Results-3\">Example Results</a></span></li><li><span><a href=\"#Application-to-Seoul-Bike-Sharing-Demand-Data\" data-toc-modified-id=\"Application-to-Seoul-Bike-Sharing-Demand-Data-4\">Application to Seoul Bike Sharing Demand Data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will complete the implementation of the `NeuralNetwork` class, starting with the code included in the next code cell.  Your implementation must meet the requirements described in the doc-strings.\n",
    "\n",
    "Run the code in [05 Optimizers](https://www.cs.colostate.edu/~anderson/cs545/notebooks/05%20Optimizers.ipynb) to create the file `optimizers.py` for use in this assignment.\n",
    "\n",
    "Then apply your `NeuralNetwork` class to the problem of predicting the value of houses in Boston as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for `NeuralNetwork` Class Saved in File `neuralnetworkA3.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting neuralnetworkA3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile neuralnetworkA3.py\n",
    "\n",
    "import numpy as np\n",
    "import optimizers as opt\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A class that represents a neural network for nonlinear regression.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_inputs : int\n",
    "        The number of values in each sample\n",
    "    n_hidden_units_by_layers : list of ints, or empty\n",
    "        The number of units in each hidden layer.\n",
    "        Its length specifies the number of hidden layers.\n",
    "    n_outputs : int\n",
    "        The number of units in output layer\n",
    "    all_weights : one-dimensional numpy array\n",
    "        Contains all weights of the network as a vector\n",
    "    Ws : list of two-dimensional numpy arrays\n",
    "        Contains matrices of weights in each layer,\n",
    "        as views into all_weights\n",
    "    all_gradients : one-dimensional numpy array\n",
    "        Contains all gradients of mean square error with\n",
    "        respect to each weight in the network as a vector\n",
    "    Grads : list of two-dimensional numpy arrays\n",
    "        Contains matrices of gradients weights in each layer,\n",
    "        as views into all_gradients\n",
    "    performance_trace : list of floats\n",
    "        Mean square error (unstandardized) after each epoch\n",
    "    n_epochs : int\n",
    "        Number of epochs trained so far\n",
    "    X_means : one-dimensional numpy array\n",
    "        Means of the components, or features, across samples\n",
    "    X_stds : one-dimensional numpy array\n",
    "        Standard deviations of the components, or features, across samples\n",
    "    T_means : one-dimensional numpy array\n",
    "        Means of the components of the targets, across samples\n",
    "    T_stds : one-dimensional numpy array\n",
    "        Standard deviations of the components of the targets, across samples\n",
    "        \n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    make_weights_and_views(shapes)\n",
    "        Creates all initial weights and views for each layer\n",
    "\n",
    "    train(X, T, n_epochs, method='sgd', learning_rate=None, verbose=True)\n",
    "        Trains the network using input and target samples by rows in X and T\n",
    "\n",
    "    use(X)\n",
    "        Applies network to inputs X and returns network's output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden_units_by_layers, n_outputs):\n",
    "        \"\"\"Creates a neural network with the given structure\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            The number of values in each sample\n",
    "        n_hidden_units_by_layers : list of ints, or empty\n",
    "            The number of units in each hidden layer.\n",
    "            Its length specifies the number of hidden layers.\n",
    "        n_outputs : int\n",
    "            The number of units in output layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NeuralNetwork object\n",
    "        \"\"\"\n",
    "\n",
    "        # Assign attribute values. \n",
    "        # Set performance_trace to [].\n",
    "        # Set self.X_means to None to indicate\n",
    "        # that standardization parameters have not been calculated.\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden_units_by_layers = n_hidden_units_by_layers\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.performance_trace = []\n",
    "        \n",
    "        self.n_epochs = 0\n",
    "        self.rho = None\n",
    "        \n",
    "        self.X_means = None\n",
    "        self.X_stds = None\n",
    "        self.T_means = None\n",
    "        self.T_stds = None\n",
    "  \n",
    "\n",
    "        # Build list of shapes for weight matrices in each layer\n",
    "        if n_hidden_units_by_layers:\n",
    "            # 1st layer weight shape\n",
    "            shapes = [(n_inputs+1, n_hidden_units_by_layers[0])]  \n",
    "            # 2nd to n-1 layer weight shape\n",
    "            for i in range(len(n_hidden_units_by_layers)-1):\n",
    "                shapes.append((n_hidden_units_by_layers[i]+1, n_hidden_units_by_layers[i+1])) \n",
    "            # last (n) layer weight\n",
    "            shapes.append((n_hidden_units_by_layers[-1]+1, n_outputs))  \n",
    "            \n",
    "        else: \n",
    "            shapes = [(n_inputs+1, n_outputs)]  # shape of weight if no hidden layer\n",
    "        \n",
    "            \n",
    "        # Call make_weights_and_views to create all_weights and Ws\n",
    "        \n",
    "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
    "\n",
    "        # Call make_weights_and_views to create all_gradients and Grads\n",
    "        \n",
    "        self.all_gradients, self.Grads = self.make_weights_and_views(shapes)\n",
    "\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        \"\"\"Creates vector of all weights and views for each layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shapes : list of pairs of ints\n",
    "            Each pair is number of rows and columns of weights in each layer.\n",
    "            Number of rows is number of inputs to layer (including constant 1).\n",
    "            Number of columns is number of units, or outputs, in layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Vector of all weights, and list of views into this vector for each layer\n",
    "        \"\"\"\n",
    "        # Create one-dimensional numpy array of all weights with random initial values\n",
    "        num_in_weights = sum([np.prod(s) for s in shapes])\n",
    "        all_weights = np.random.uniform(-1, 1, size=(num_in_weights, 1)).flatten()\n",
    "\n",
    "        # Build weight matrices as list of views (pairs of number of rows and number \n",
    "        # of columns) by reshaping corresponding elements from vector of all weights \n",
    "        # into correct shape for each layer. \n",
    "        first_idx = 0\n",
    "        Ws = []\n",
    "        for shp in shapes:\n",
    "            nw = shp[0] * shp[1]\n",
    "            last_idx = first_idx + nw\n",
    "            Ws.append(all_weights[first_idx:last_idx].reshape(shp))\n",
    "            first_idx = last_idx\n",
    "        \n",
    "        # Divide values of each weight matrix by square root of number of its inputs.\n",
    "        for w in Ws:\n",
    "            w[:] /= np.sqrt(w.shape[0] + 1)\n",
    "        \n",
    "        # Set output layer weights to zero.\n",
    "        Ws[-1][:] = np.zeros(shapes[-1])\n",
    "        \n",
    "        return all_weights, Ws\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'NeuralNetwork({}, {}, {})'.format(self.n_inputs, self.n_hidden_units_by_layers, self.n_outputs)\n",
    "\n",
    "    def __str__(self):\n",
    "        s = self.__repr__()\n",
    "        if self.n_epochs > 0:  # self.total_epochs\n",
    "            s += '\\n Trained for {} epochs.'.format(self.n_epochs)\n",
    "            s += '\\n Final standardized training error {:.4g}.'.format(self.performance_trace[-1])\n",
    "        return s\n",
    " \n",
    "    def train(self, X, T, n_epochs, method='sgd', learning_rate=None, verbose=True):\n",
    "        \"\"\"Updates the weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array \n",
    "            number of samples  by  number of input components\n",
    "        T : two-dimensional numpy array\n",
    "            number of samples  by  number of output components\n",
    "        n_epochs : int\n",
    "            Number of passes to take through all samples\n",
    "        method : str\n",
    "            'sgd', 'adam', or 'scg'\n",
    "        learning_rate : float\n",
    "            Controls the step size of each update, only for sgd and adam\n",
    "        verbose: boolean\n",
    "            If True, progress is shown with print statements\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Calculate and assign standardization parameters\n",
    "        self.X_means = X.mean(axis = 0)\n",
    "        self.X_stds = X.std(axis = 0)\n",
    "        self.T_means = T.mean(axis = 0)\n",
    "        self.T_stds = T.std(axis = 0)\n",
    "\n",
    "        # Standardize X and T.  Assign back to X and T.\n",
    "        X = (X - self.X_means) / self.X_stds\n",
    "        T = (T - self.T_means) / self.T_stds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = opt.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert mean-square error to root-mean-square error,\n",
    "        # Here we use a lambda function just to illustrate its use.  \n",
    "        # We could have also defined this function with\n",
    "        # def error_convert_f(err):\n",
    "        #     return np.sqrt(err)\n",
    "\n",
    "        error_convert_f = lambda err: np.sqrt(err) \n",
    "        \n",
    "        # Call the requested optimizer method to train the weights.\n",
    "        if method == 'sgd':\n",
    "            \n",
    "            performance_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                              fargs=[X, T], n_epochs=n_epochs,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              error_convert_f=error_convert_f, \n",
    "                                              error_convert_name='RMSE',\n",
    "                                              verbose=verbose)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            performance_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                               fargs=[X, T], n_epochs=n_epochs,\n",
    "                                               learning_rate=learning_rate,\n",
    "                                               error_convert_f=error_convert_f, \n",
    "                                               error_convert_name='RMSE',\n",
    "                                               verbose=verbose)\n",
    "\n",
    "        elif method == 'scg':\n",
    "\n",
    "            performance_trace = optimizer.scg(self.error_f, self.gradient_f,\n",
    "                                              fargs=[X, T], n_epochs=n_epochs,\n",
    "                                              error_convert_f=error_convert_f, \n",
    "                                              error_convert_name='RMSE',\n",
    "                                              verbose=verbose)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd', 'adam', or 'scg'\")\n",
    "\n",
    "        self.n_epochs += len(performance_trace)\n",
    "        self.performance_trace += performance_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods\n",
    "        # after training, such as:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _add_ones(self, X):\n",
    "        return np.insert(X, 0, 1, 1)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"Calculate outputs of each layer given inputs in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : input samples, standardized.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Standardized outputs of all layers as list, include X as first element.\n",
    "        \"\"\"        \n",
    "        self.Zs = [X]\n",
    "        # Append output of each layer to list in self.Zs, then return it.\n",
    "        weights = self.Ws\n",
    "        Z = X\n",
    "        # for layers upto n-1\n",
    "        for i in range(len(weights) - 1):\n",
    "            Z_w = np.tanh(self._add_ones(Z) @ weights[i])               \n",
    "            Z = Z_w\n",
    "            self.Zs.append(Z_w)\n",
    "            \n",
    "        # for output layer\n",
    "        if self.n_hidden_units_by_layers:\n",
    "            self.Zs.append(self._add_ones(Z_w) @ weights[-1]) \n",
    "        else:\n",
    "            self.Zs.append(self._add_ones(X) @ weights[-1])\n",
    "            \n",
    "        return self.Zs\n",
    "        \n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        \"\"\"Calculate output of net given input X and its mean squared error.\n",
    "        Function to be minimized by optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array, standardized\n",
    "            number of samples  by  number of input components\n",
    "        T : two-dimensional numpy array, standardized\n",
    "            number of samples  by  number of output components\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Standardized mean square error as scalar float that is the mean\n",
    "        square error over all samples and all network outputs.\n",
    "        \"\"\"\n",
    "        # Call _forward, calculate mean square error and return it.\n",
    "        Y = self._forward(X)[-1]\n",
    "        mse = np.mean(((T - Y)) **2)\n",
    "        return mse\n",
    "         \n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        \"\"\"Returns gradient wrt all weights. Assumes _forward already called\n",
    "        so input and all layer outputs stored in self.Zs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array, standardized\n",
    "            number of samples  x  number of input components\n",
    "        T : two-dimensional numpy array, standardized\n",
    "            number of samples  x  number of output components\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Vector of gradients of mean square error wrt all weights\n",
    "        \"\"\"\n",
    "        # Assumes forward_pass just called with layer outputs saved in self.Zs.\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        n_layers = len(self.n_hidden_units_by_layers) + 1\n",
    "\n",
    "        # delta is delta matrix to be back propagated.\n",
    "        # Dividing by n_samples and n_outputs here replaces the scaling of the learning rate.\n",
    "        \n",
    "        delta = (T - self.Zs[-1]) / (n_samples * n_outputs)\n",
    "\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        \n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            \n",
    "            # gradient of all but bias weights\n",
    "            Z = self.Zs[layeri]\n",
    "            W = self.Ws[layeri]\n",
    "            self.Grads[layeri][:] = -(self._add_ones(Z).T @ delta)\n",
    "            \n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if layeri > 0:\n",
    "                delta = delta @ W[1:, :].T * (1 - Z ** 2)\n",
    "            \n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self, X):\n",
    "        \"\"\"Return the output of the network for input samples as rows in X.\n",
    "        X assumed to not be standardized.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : two-dimensional numpy array\n",
    "            number of samples  by  number of input components, unstandardized\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output of neural network, unstandardized, as numpy array\n",
    "        of shape  number of samples  by  number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        # Standardize X\n",
    "        Xs = (X - self.X_means) / self.X_stds\n",
    "        \n",
    "        Ys = self._forward(Xs)[-1]\n",
    "        # Unstandardize output Y before returning it\n",
    "        Y = Ys * self.T_stds + self.T_means\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    def get_performance_trace(self):\n",
    "        \"\"\"Returns list of unstandardized root-mean square error for each epoch\"\"\"\n",
    "        return self.performance_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the `NeuralNetwork` class with some simple data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import neuralnetworkA3 as nn  # Your file produced from the above code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd: Epoch 1 RMSE=1.00005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-1.51603124e-07,  9.99801782e-04],\n",
       "        [ 2.00719909e-03,  3.00940664e-03]]),\n",
       " array([[0.00398889, 0.00498788],\n",
       "        [0.00600106, 0.00700115],\n",
       "        [0.00800157, 0.00900172]]),\n",
       " array([[0.00898958],\n",
       "        [0.01099768],\n",
       "        [0.01199691]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(0, 2, 0.5).reshape(-1, 1)\n",
    "T = np.sin(X) * np.sin(X * 10)\n",
    "\n",
    "nnet = nn.NeuralNetwork(X.shape[1], [2, 2], 1)\n",
    "    \n",
    "# Set all weights here to allow comparison of your calculations\n",
    "# Must use [:] to overwrite values in all_weights.\n",
    "# Without [:], new array is assigned to self.all_weights, so self.Ws no longer refer to same memory\n",
    "nnet.all_weights[:] = np.arange(len(nnet.all_weights)) * 0.001\n",
    "\n",
    "nnet.train(X, T, n_epochs=1, method='sgd', learning_rate=0.1)\n",
    "\n",
    "nnet.Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.34164079],\n",
       "        [-0.4472136 ],\n",
       "        [ 0.4472136 ],\n",
       "        [ 1.34164079]]),\n",
       " array([[-0.00268328, -0.00302491],\n",
       "        [-0.00089443, -0.00034164],\n",
       "        [ 0.00089443,  0.00234164],\n",
       "        [ 0.00268328,  0.00502488]]),\n",
       " array([[0.00395968, 0.00495395],\n",
       "        [0.00399188, 0.00499062],\n",
       "        [0.00402408, 0.00502729],\n",
       "        [0.00405628, 0.00506396]]),\n",
       " array([[0.010103  ],\n",
       "        [0.0101038 ],\n",
       "        [0.01010459],\n",
       "        [0.01010539]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.51603124e-06,  1.98217653e-06],\n",
       "        [-7.19909102e-05, -9.40663907e-05]]),\n",
       " array([[ 1.11145896e-04,  1.21249680e-04],\n",
       "        [-1.05587719e-05, -1.15185542e-05],\n",
       "        [-1.57268813e-05, -1.71564392e-05]]),\n",
       " array([[1.01041953e-02],\n",
       "        [2.32194989e-05],\n",
       "        [3.09340617e-05]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06308723],\n",
       "       [-0.06308687],\n",
       "       [-0.06308651],\n",
       "       [-0.06308615]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = nnet.use(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================\n",
      "method is sgd and rho is 0.05\n",
      "=========================================\n",
      "\n",
      "sgd: Epoch 1000 RMSE=1.00000\n",
      "sgd: Epoch 2000 RMSE=1.00000\n",
      "sgd: Epoch 3000 RMSE=0.99539\n",
      "sgd: Epoch 4000 RMSE=0.97261\n",
      "sgd: Epoch 5000 RMSE=0.68183\n",
      "sgd: Epoch 6000 RMSE=0.55867\n",
      "sgd: Epoch 7000 RMSE=0.54350\n",
      "sgd: Epoch 8000 RMSE=0.43648\n",
      "sgd: Epoch 9000 RMSE=0.40719\n",
      "sgd: Epoch 10000 RMSE=0.39497\n",
      "\n",
      "=========================================\n",
      "method is adam and rho is 0.02\n",
      "=========================================\n",
      "\n",
      "Adam: Epoch 1000 RMSE=0.83076\n",
      "Adam: Epoch 2000 RMSE=0.64750\n",
      "Adam: Epoch 3000 RMSE=0.64601\n",
      "Adam: Epoch 4000 RMSE=0.64567\n",
      "Adam: Epoch 5000 RMSE=0.64550\n",
      "Adam: Epoch 6000 RMSE=0.64540\n",
      "Adam: Epoch 7000 RMSE=0.64532\n",
      "Adam: Epoch 8000 RMSE=0.64527\n",
      "Adam: Epoch 9000 RMSE=0.65070\n",
      "Adam: Epoch 10000 RMSE=0.64520\n",
      "\n",
      "=========================================\n",
      "method is scg and rho is None\n",
      "=========================================\n",
      "\n",
      "SCG: Iteration 1000 RMSE=0.64487\n",
      "SCG: Iteration 2000 RMSE=0.46553\n",
      "SCG: Iteration 3000 RMSE=0.45547\n",
      "SCG: Iteration 4000 RMSE=0.32589\n",
      "SCG: Iteration 5000 RMSE=0.30054\n",
      "SCG: Iteration 6000 RMSE=0.28646\n",
      "SCG: Iteration 7000 RMSE=0.28084\n",
      "SCG: Iteration 8000 RMSE=0.27979\n",
      "SCG: Iteration 9000 RMSE=0.27917\n",
      "SCG: Iteration 10000 RMSE=0.27835\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(0, 2, 0.05).reshape(-1, 1)\n",
    "T = np.sin(X) * np.sin(X * 10)\n",
    "\n",
    "n_epochs = 10000\n",
    "method_rhos = [('sgd', 0.05),\n",
    "               ('adam', 0.02),\n",
    "               ('scg', None)]\n",
    "errors = []\n",
    "for method, rho in method_rhos:\n",
    "    \n",
    "    print('\\n=========================================')\n",
    "    print(f'method is {method} and rho is {rho}')\n",
    "    print('=========================================\\n')\n",
    "\n",
    "    nnet = nn.NeuralNetwork(X.shape[1], [2, 2], 1)\n",
    "    \n",
    "    # Set all weights here to allow comparison of your calculations\n",
    "    # Must use [:] to overwrite values in all_weights.\n",
    "    # Without [:], new array is assigned to self.all_weights, so self.Ws no longer refer to same memory\n",
    "    nnet.all_weights[:] = np.arange(len(nnet.all_weights)) * 0.001\n",
    "    \n",
    "    nnet.train(X, T, n_epochs, method=method, learning_rate=rho)\n",
    "    Y = nnet.use(X)\n",
    "    errors.append(nnet.get_performance_trace())\n",
    "    plt.plot(X, Y, 'o-', label='Model ' + method)\n",
    "\n",
    "plt.plot(X, T, 'o', label='Train')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('T or Y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for error_trace in errors:\n",
    "    plt.plot(error_trace)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Standardized error')\n",
    "plt.legend([mr[0] for mr in method_rhos]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Seoul Bike Sharing Demand Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [bike-sharing.csv](https://www.cs.colostate.edu/~anderson/cs545/notebooks/bike-sharing.csv).  This is data modified very slightly from [UC Irvine ML Repo](https://archive-beta.ics.uci.edu/ml/datasets/seoul+bike+sharing+demand#Abstract). Read it into python using the `pandas.read_csv` function.  Assign `X` and `T` as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8760, 9), (8760, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('SeoulBikeData.csv')\n",
    "T = data['Rented Bike Count'].to_numpy().reshape(-1, 1)\n",
    "X = data[['Hour', 'Temperature(C)', 'Humidity(%)',\n",
    "          'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(C)',\n",
    "          'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']].to_numpy()\n",
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training your neural networks, partition the data into training and testing partitions, as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def partition(X, T, train_fraction):\n",
    "    \"\"\"\n",
    "    Creates train and test datasets.\n",
    "    \n",
    "    :param X: 2D numpy array.\n",
    "              The input variables of the model.\n",
    "    :param T: 2D numpy array\n",
    "              The output variable of the model.\n",
    "    :param X: float\n",
    "              fraction of data in training set.\n",
    "              The input variables of the model.\n",
    "    \n",
    "    return: Xtrain, Ttrain, Xtest, Ttest\n",
    "    \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    rows = np.arange(n_samples)\n",
    "    np.random.shuffle(rows)\n",
    "\n",
    "    n_train = round(n_samples * train_fraction)\n",
    "    Xtrain = X[rows[:n_train], :]\n",
    "    Ttrain = T[rows[:n_train], :]\n",
    "    Xtest = X[rows[n_train:], :]\n",
    "    Ttest = T[rows[n_train:], :]\n",
    "\n",
    "    return Xtrain, Ttrain, Xtest, Ttest\n",
    "\n",
    "def rmse(T, Y):\n",
    "    return np.sqrt(np.mean((T - Y)**2))\n",
    "\n",
    "def make_param_grid(structure, rho, epoch):\n",
    "    \"\"\"\n",
    "    Makes a gridspace with all possible combinations of input parameters.\n",
    "    \n",
    "    :param structure: list of int\n",
    "                      Number of units in each hidden layers. Each element of the list represents one hidden layer.\n",
    "    :param rho: float\n",
    "                Learning rate of the model.\n",
    "    :param epoch: int\n",
    "                  Number of epoch to run the model.\n",
    "                  \n",
    "    :return: A list of all parameter combinations.\n",
    "    \"\"\"\n",
    "    param_combs = list(itertools.product(*[structure, rho, epoch]))\n",
    "    return param_combs\n",
    "\n",
    "\n",
    "def configure_nnet_model(X, T, structures, rhos, epochs, method, train_test_ratio):\n",
    "    \"\"\"\n",
    "    Runs the model for number of times of parameter combinations and saves model layout, train rmse, and test rmse. \n",
    "    \n",
    "    :param X: 2D numpy array.\n",
    "              The input variables of the model.\n",
    "    :param T: 2D numpy array\n",
    "              The output variable of the model.\n",
    "    :param structures: A nested list of int, such as [[50, 20], [30, 10]]\n",
    "                       Number of units in each hidden layers. Each element of the list represents one hidden layer.\n",
    "    :param rhos: list of float\n",
    "                 Learning rate of the model.\n",
    "    :param epochs: list of int\n",
    "                   Number of epoch to run the model.\n",
    "    :param method : str\n",
    "                    Optimization method. Accepted str from - 'sgd', 'adam', or 'scg'.\n",
    "    :param train_test_ratio: float\n",
    "                             ratio of dataset in training.\n",
    "    \n",
    "    :return: A dataframe with model layouts and respective train test rmse.\n",
    "    \"\"\"\n",
    "    import neuralnetworkA3 as nn\n",
    "    \n",
    "    parameter_combinations = make_param_grid(structures, rhos, epochs)\n",
    "\n",
    "    Xtrain, Ttrain, Xtest, Ttest = partition(X, T, train_test_ratio)\n",
    "\n",
    "    n_inputs, n_outputs = Xtrain.shape[1], Ttrain.shape[1]\n",
    "\n",
    "    result_dict = {'Method': [], 'Structure': [], 'Epochs': [], 'Learning Rate': [], 'Train RMSE': [], \n",
    "                   'Test RMSE': []}\n",
    "\n",
    "    for params in parameter_combinations:\n",
    "        hidden_layers, rho, epoch = params\n",
    "        nn_model = nn.NeuralNetwork(n_inputs, hidden_layers, n_outputs)\n",
    "        nn_model.train(Xtrain, Ttrain, n_epochs=epoch, method=method, learning_rate=rho, verbose=False)\n",
    "        \n",
    "        # RMSE (No standardization is required)\n",
    "        Ttrain_predicted = nn_model.use(Xtrain)\n",
    "        rmse_train = rmse(Ttrain_predicted, Ttrain)\n",
    "        \n",
    "        Ttest_predicted = nn_model.use(Xtest)\n",
    "        rmse_test = rmse(Ttest_predicted, Ttest)\n",
    "        \n",
    "        print(f'Training for {method=}, {hidden_layers=}, {rho=}, {epoch=}, train rmse={rmse_train:.2f}, test rmse={rmse_test:.2f}')\n",
    "        \n",
    "        # appending results to the result dictionary\n",
    "        result_dict['Method'].append(method)\n",
    "        result_dict['Structure'].append(hidden_layers)\n",
    "        result_dict['Epochs'].append(epoch)\n",
    "        result_dict['Learning Rate'].append(rho)\n",
    "        result_dict['Train RMSE'].append(rmse_train)\n",
    "        result_dict['Test RMSE'].append(rmse_test)\n",
    "\n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "    result_df.sort_values(by=['Test RMSE'], axis=0, ascending=True, inplace=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7008, 9), (7008, 1), (1752, 9), (1752, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have assigned `X` and `T` correctly.\n",
    "\n",
    "Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8)\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for best model layout for Seoul Bike data\n",
    "nnet_structure = [[10, 10, 5, 5], [10, 5], [30, 10]]\n",
    "nnet_rho = [0.001, 0.005, 0.01]\n",
    "nnet_epoch = [2000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=2000, train rmse=463.49, test rmse=436.36\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=5000, train rmse=430.09, test rmse=402.43\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=10000, train rmse=389.34, test rmse=367.14\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=2000, train rmse=401.90, test rmse=375.73\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=5000, train rmse=374.79, test rmse=358.02\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=10000, train rmse=332.06, test rmse=327.78\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=2000, train rmse=370.88, test rmse=354.33\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=5000, train rmse=331.14, test rmse=324.14\n",
      "Training for method='sgd', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=10000, train rmse=319.87, test rmse=318.03\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.001, epoch=2000, train rmse=453.26, test rmse=426.26\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.001, epoch=5000, train rmse=431.44, test rmse=404.07\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.001, epoch=10000, train rmse=407.97, test rmse=381.55\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.005, epoch=2000, train rmse=405.58, test rmse=377.64\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.005, epoch=5000, train rmse=366.34, test rmse=349.58\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.005, epoch=10000, train rmse=360.16, test rmse=347.50\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.01, epoch=2000, train rmse=386.60, test rmse=363.26\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.01, epoch=5000, train rmse=353.28, test rmse=338.58\n",
      "Training for method='sgd', hidden_layers=[10, 5], rho=0.01, epoch=10000, train rmse=331.96, test rmse=327.61\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.001, epoch=2000, train rmse=451.27, test rmse=422.95\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.001, epoch=5000, train rmse=428.92, test rmse=401.00\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.001, epoch=10000, train rmse=399.83, test rmse=372.91\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.005, epoch=2000, train rmse=401.73, test rmse=375.64\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.005, epoch=5000, train rmse=366.35, test rmse=347.80\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.005, epoch=10000, train rmse=334.02, test rmse=326.91\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.01, epoch=2000, train rmse=376.04, test rmse=350.74\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.01, epoch=5000, train rmse=333.95, test rmse=331.05\n",
      "Training for method='sgd', hidden_layers=[30, 10], rho=0.01, epoch=10000, train rmse=318.37, test rmse=318.14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>319.866495</td>\n",
       "      <td>318.030270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>318.373095</td>\n",
       "      <td>318.136429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>331.141614</td>\n",
       "      <td>324.143791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>334.023930</td>\n",
       "      <td>326.909262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>331.961662</td>\n",
       "      <td>327.605815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>332.055712</td>\n",
       "      <td>327.778177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>333.953754</td>\n",
       "      <td>331.045215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>353.283023</td>\n",
       "      <td>338.579692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>360.156805</td>\n",
       "      <td>347.498866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>366.353687</td>\n",
       "      <td>347.796469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>366.344237</td>\n",
       "      <td>349.583910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>376.040017</td>\n",
       "      <td>350.735085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>370.881135</td>\n",
       "      <td>354.327851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>374.785519</td>\n",
       "      <td>358.017412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>386.597858</td>\n",
       "      <td>363.264264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>389.336938</td>\n",
       "      <td>367.140980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>399.832455</td>\n",
       "      <td>372.912366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>401.727512</td>\n",
       "      <td>375.636651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>401.901248</td>\n",
       "      <td>375.727675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>405.581688</td>\n",
       "      <td>377.643865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>407.968590</td>\n",
       "      <td>381.551163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>428.923033</td>\n",
       "      <td>401.002769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>430.090385</td>\n",
       "      <td>402.431190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>431.438202</td>\n",
       "      <td>404.073457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>451.272085</td>\n",
       "      <td>422.947747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>453.262602</td>\n",
       "      <td>426.259777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sgd</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>463.494382</td>\n",
       "      <td>436.361380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Method       Structure  Epochs  Learning Rate  Train RMSE   Test RMSE\n",
       "8     sgd  [10, 10, 5, 5]   10000          0.010  319.866495  318.030270\n",
       "26    sgd        [30, 10]   10000          0.010  318.373095  318.136429\n",
       "7     sgd  [10, 10, 5, 5]    5000          0.010  331.141614  324.143791\n",
       "23    sgd        [30, 10]   10000          0.005  334.023930  326.909262\n",
       "17    sgd         [10, 5]   10000          0.010  331.961662  327.605815\n",
       "5     sgd  [10, 10, 5, 5]   10000          0.005  332.055712  327.778177\n",
       "25    sgd        [30, 10]    5000          0.010  333.953754  331.045215\n",
       "16    sgd         [10, 5]    5000          0.010  353.283023  338.579692\n",
       "14    sgd         [10, 5]   10000          0.005  360.156805  347.498866\n",
       "22    sgd        [30, 10]    5000          0.005  366.353687  347.796469\n",
       "13    sgd         [10, 5]    5000          0.005  366.344237  349.583910\n",
       "24    sgd        [30, 10]    2000          0.010  376.040017  350.735085\n",
       "6     sgd  [10, 10, 5, 5]    2000          0.010  370.881135  354.327851\n",
       "4     sgd  [10, 10, 5, 5]    5000          0.005  374.785519  358.017412\n",
       "15    sgd         [10, 5]    2000          0.010  386.597858  363.264264\n",
       "2     sgd  [10, 10, 5, 5]   10000          0.001  389.336938  367.140980\n",
       "20    sgd        [30, 10]   10000          0.001  399.832455  372.912366\n",
       "21    sgd        [30, 10]    2000          0.005  401.727512  375.636651\n",
       "3     sgd  [10, 10, 5, 5]    2000          0.005  401.901248  375.727675\n",
       "12    sgd         [10, 5]    2000          0.005  405.581688  377.643865\n",
       "11    sgd         [10, 5]   10000          0.001  407.968590  381.551163\n",
       "19    sgd        [30, 10]    5000          0.001  428.923033  401.002769\n",
       "1     sgd  [10, 10, 5, 5]    5000          0.001  430.090385  402.431190\n",
       "10    sgd         [10, 5]    5000          0.001  431.438202  404.073457\n",
       "18    sgd        [30, 10]    2000          0.001  451.272085  422.947747\n",
       "9     sgd         [10, 5]    2000          0.001  453.262602  426.259777\n",
       "0     sgd  [10, 10, 5, 5]    2000          0.001  463.494382  436.361380"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_results = configure_nnet_model(X, T, nnet_structure, nnet_rho, nnet_epoch, method='sgd', train_test_ratio=0.8)\n",
    "sgd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=2000, train rmse=316.05, test rmse=319.17\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=5000, train rmse=293.72, test rmse=308.62\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=10000, train rmse=281.83, test rmse=320.03\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=2000, train rmse=304.38, test rmse=318.85\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=5000, train rmse=288.90, test rmse=307.01\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=10000, train rmse=271.02, test rmse=320.37\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=2000, train rmse=290.35, test rmse=314.27\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=5000, train rmse=280.45, test rmse=317.21\n",
      "Training for method='adam', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=10000, train rmse=274.21, test rmse=314.87\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.001, epoch=2000, train rmse=354.60, test rmse=346.04\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.001, epoch=5000, train rmse=312.52, test rmse=315.53\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.001, epoch=10000, train rmse=308.98, test rmse=317.70\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.005, epoch=2000, train rmse=310.01, test rmse=316.52\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.005, epoch=5000, train rmse=311.11, test rmse=315.93\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.005, epoch=10000, train rmse=297.80, test rmse=314.25\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.01, epoch=2000, train rmse=308.78, test rmse=310.54\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.01, epoch=5000, train rmse=305.42, test rmse=310.79\n",
      "Training for method='adam', hidden_layers=[10, 5], rho=0.01, epoch=10000, train rmse=301.08, test rmse=317.23\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.001, epoch=2000, train rmse=313.21, test rmse=319.47\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.001, epoch=5000, train rmse=264.00, test rmse=322.22\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.001, epoch=10000, train rmse=253.97, test rmse=336.36\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.005, epoch=2000, train rmse=267.75, test rmse=321.55\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.005, epoch=5000, train rmse=253.43, test rmse=339.90\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.005, epoch=10000, train rmse=246.10, test rmse=333.35\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.01, epoch=2000, train rmse=266.12, test rmse=326.72\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.01, epoch=5000, train rmse=250.65, test rmse=333.80\n",
      "Training for method='adam', hidden_layers=[30, 10], rho=0.01, epoch=10000, train rmse=245.02, test rmse=334.59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>288.897798</td>\n",
       "      <td>307.010033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>293.722600</td>\n",
       "      <td>308.618803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>308.784042</td>\n",
       "      <td>310.535040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>305.417810</td>\n",
       "      <td>310.794719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>297.802566</td>\n",
       "      <td>314.252859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>290.348266</td>\n",
       "      <td>314.267240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>274.212954</td>\n",
       "      <td>314.869790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>312.520360</td>\n",
       "      <td>315.525442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>311.107880</td>\n",
       "      <td>315.926237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>310.012628</td>\n",
       "      <td>316.517829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>280.448010</td>\n",
       "      <td>317.214707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>301.075594</td>\n",
       "      <td>317.228947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>308.984131</td>\n",
       "      <td>317.699791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>304.384682</td>\n",
       "      <td>318.851948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>316.052224</td>\n",
       "      <td>319.172403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>313.206111</td>\n",
       "      <td>319.470560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>281.830804</td>\n",
       "      <td>320.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>271.024529</td>\n",
       "      <td>320.367948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>267.752561</td>\n",
       "      <td>321.551226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>264.004005</td>\n",
       "      <td>322.221802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>266.115887</td>\n",
       "      <td>326.717823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>246.095554</td>\n",
       "      <td>333.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>250.650266</td>\n",
       "      <td>333.804304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>245.019637</td>\n",
       "      <td>334.588055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>253.965670</td>\n",
       "      <td>336.361435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>adam</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>253.427293</td>\n",
       "      <td>339.895645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>354.595809</td>\n",
       "      <td>346.039047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Method       Structure  Epochs  Learning Rate  Train RMSE   Test RMSE\n",
       "4    adam  [10, 10, 5, 5]    5000          0.005  288.897798  307.010033\n",
       "1    adam  [10, 10, 5, 5]    5000          0.001  293.722600  308.618803\n",
       "15   adam         [10, 5]    2000          0.010  308.784042  310.535040\n",
       "16   adam         [10, 5]    5000          0.010  305.417810  310.794719\n",
       "14   adam         [10, 5]   10000          0.005  297.802566  314.252859\n",
       "6    adam  [10, 10, 5, 5]    2000          0.010  290.348266  314.267240\n",
       "8    adam  [10, 10, 5, 5]   10000          0.010  274.212954  314.869790\n",
       "10   adam         [10, 5]    5000          0.001  312.520360  315.525442\n",
       "13   adam         [10, 5]    5000          0.005  311.107880  315.926237\n",
       "12   adam         [10, 5]    2000          0.005  310.012628  316.517829\n",
       "7    adam  [10, 10, 5, 5]    5000          0.010  280.448010  317.214707\n",
       "17   adam         [10, 5]   10000          0.010  301.075594  317.228947\n",
       "11   adam         [10, 5]   10000          0.001  308.984131  317.699791\n",
       "3    adam  [10, 10, 5, 5]    2000          0.005  304.384682  318.851948\n",
       "0    adam  [10, 10, 5, 5]    2000          0.001  316.052224  319.172403\n",
       "18   adam        [30, 10]    2000          0.001  313.206111  319.470560\n",
       "2    adam  [10, 10, 5, 5]   10000          0.001  281.830804  320.025900\n",
       "5    adam  [10, 10, 5, 5]   10000          0.005  271.024529  320.367948\n",
       "21   adam        [30, 10]    2000          0.005  267.752561  321.551226\n",
       "19   adam        [30, 10]    5000          0.001  264.004005  322.221802\n",
       "24   adam        [30, 10]    2000          0.010  266.115887  326.717823\n",
       "23   adam        [30, 10]   10000          0.005  246.095554  333.345900\n",
       "25   adam        [30, 10]    5000          0.010  250.650266  333.804304\n",
       "26   adam        [30, 10]   10000          0.010  245.019637  334.588055\n",
       "20   adam        [30, 10]   10000          0.001  253.965670  336.361435\n",
       "22   adam        [30, 10]    5000          0.005  253.427293  339.895645\n",
       "9    adam         [10, 5]    2000          0.001  354.595809  346.039047"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam_results = configure_nnet_model(X, T, nnet_structure, nnet_rho, nnet_epoch, method='adam', train_test_ratio=0.8)\n",
    "adam_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=2000, train rmse=287.85, test rmse=314.50\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=5000, train rmse=275.70, test rmse=315.46\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.001, epoch=10000, train rmse=273.39, test rmse=324.02\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=2000, train rmse=286.01, test rmse=309.20\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=5000, train rmse=270.35, test rmse=318.72\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.005, epoch=10000, train rmse=268.76, test rmse=321.07\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=2000, train rmse=296.78, test rmse=309.38\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=5000, train rmse=269.09, test rmse=306.13\n",
      "Training for method='scg', hidden_layers=[10, 10, 5, 5], rho=0.01, epoch=10000, train rmse=267.46, test rmse=342.46\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.001, epoch=2000, train rmse=298.50, test rmse=324.27\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.001, epoch=5000, train rmse=304.02, test rmse=323.08\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.001, epoch=10000, train rmse=301.39, test rmse=318.23\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.005, epoch=2000, train rmse=304.07, test rmse=313.32\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.005, epoch=5000, train rmse=304.91, test rmse=321.84\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.005, epoch=10000, train rmse=299.95, test rmse=325.34\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.01, epoch=2000, train rmse=303.85, test rmse=325.87\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.01, epoch=5000, train rmse=299.10, test rmse=314.81\n",
      "Training for method='scg', hidden_layers=[10, 5], rho=0.01, epoch=10000, train rmse=296.36, test rmse=317.78\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.001, epoch=2000, train rmse=254.83, test rmse=334.03\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.001, epoch=5000, train rmse=242.61, test rmse=342.50\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.001, epoch=10000, train rmse=242.28, test rmse=359.59\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.005, epoch=2000, train rmse=255.57, test rmse=320.54\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.005, epoch=5000, train rmse=243.19, test rmse=357.88\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.005, epoch=10000, train rmse=232.74, test rmse=347.56\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.01, epoch=2000, train rmse=255.46, test rmse=322.40\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.01, epoch=5000, train rmse=246.05, test rmse=337.51\n",
      "Training for method='scg', hidden_layers=[30, 10], rho=0.01, epoch=10000, train rmse=237.64, test rmse=356.20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>269.088079</td>\n",
       "      <td>306.125934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>286.009221</td>\n",
       "      <td>309.198389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>296.775773</td>\n",
       "      <td>309.382869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>304.065578</td>\n",
       "      <td>313.324705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>287.849780</td>\n",
       "      <td>314.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>299.102719</td>\n",
       "      <td>314.805142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>275.697016</td>\n",
       "      <td>315.463098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>296.364172</td>\n",
       "      <td>317.777069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>301.389901</td>\n",
       "      <td>318.229280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>270.347991</td>\n",
       "      <td>318.722469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>255.571185</td>\n",
       "      <td>320.539151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>268.758693</td>\n",
       "      <td>321.068201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>304.908211</td>\n",
       "      <td>321.836342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>255.457342</td>\n",
       "      <td>322.398710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>304.022066</td>\n",
       "      <td>323.078256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>273.393234</td>\n",
       "      <td>324.018077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>298.496271</td>\n",
       "      <td>324.268234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>299.948020</td>\n",
       "      <td>325.342217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>303.849151</td>\n",
       "      <td>325.866651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>254.832704</td>\n",
       "      <td>334.028591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>246.054237</td>\n",
       "      <td>337.510795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>267.457025</td>\n",
       "      <td>342.458842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>242.611320</td>\n",
       "      <td>342.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>232.744785</td>\n",
       "      <td>347.560850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>237.638527</td>\n",
       "      <td>356.201128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>243.190582</td>\n",
       "      <td>357.884456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>scg</td>\n",
       "      <td>[30, 10]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>242.280066</td>\n",
       "      <td>359.585340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Method       Structure  Epochs  Learning Rate  Train RMSE   Test RMSE\n",
       "7     scg  [10, 10, 5, 5]    5000          0.010  269.088079  306.125934\n",
       "3     scg  [10, 10, 5, 5]    2000          0.005  286.009221  309.198389\n",
       "6     scg  [10, 10, 5, 5]    2000          0.010  296.775773  309.382869\n",
       "12    scg         [10, 5]    2000          0.005  304.065578  313.324705\n",
       "0     scg  [10, 10, 5, 5]    2000          0.001  287.849780  314.495050\n",
       "16    scg         [10, 5]    5000          0.010  299.102719  314.805142\n",
       "1     scg  [10, 10, 5, 5]    5000          0.001  275.697016  315.463098\n",
       "17    scg         [10, 5]   10000          0.010  296.364172  317.777069\n",
       "11    scg         [10, 5]   10000          0.001  301.389901  318.229280\n",
       "4     scg  [10, 10, 5, 5]    5000          0.005  270.347991  318.722469\n",
       "21    scg        [30, 10]    2000          0.005  255.571185  320.539151\n",
       "5     scg  [10, 10, 5, 5]   10000          0.005  268.758693  321.068201\n",
       "13    scg         [10, 5]    5000          0.005  304.908211  321.836342\n",
       "24    scg        [30, 10]    2000          0.010  255.457342  322.398710\n",
       "10    scg         [10, 5]    5000          0.001  304.022066  323.078256\n",
       "2     scg  [10, 10, 5, 5]   10000          0.001  273.393234  324.018077\n",
       "9     scg         [10, 5]    2000          0.001  298.496271  324.268234\n",
       "14    scg         [10, 5]   10000          0.005  299.948020  325.342217\n",
       "15    scg         [10, 5]    2000          0.010  303.849151  325.866651\n",
       "18    scg        [30, 10]    2000          0.001  254.832704  334.028591\n",
       "25    scg        [30, 10]    5000          0.010  246.054237  337.510795\n",
       "8     scg  [10, 10, 5, 5]   10000          0.010  267.457025  342.458842\n",
       "19    scg        [30, 10]    5000          0.001  242.611320  342.501400\n",
       "23    scg        [30, 10]   10000          0.005  232.744785  347.560850\n",
       "26    scg        [30, 10]   10000          0.010  237.638527  356.201128\n",
       "22    scg        [30, 10]    5000          0.005  243.190582  357.884456\n",
       "20    scg        [30, 10]   10000          0.001  242.280066  359.585340"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scg_results = configure_nnet_model(X, T, nnet_structure, nnet_rho, nnet_epoch, method='scg', train_test_ratio=0.8)\n",
    "scg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Structure</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>269.088079</td>\n",
       "      <td>306.125934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>288.897798</td>\n",
       "      <td>307.010033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>293.722600</td>\n",
       "      <td>308.618803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>286.009221</td>\n",
       "      <td>309.198389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>296.775773</td>\n",
       "      <td>309.382869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>308.784042</td>\n",
       "      <td>310.535040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>305.417810</td>\n",
       "      <td>310.794719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>304.065578</td>\n",
       "      <td>313.324705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>297.802566</td>\n",
       "      <td>314.252859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>290.348266</td>\n",
       "      <td>314.267240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>287.849780</td>\n",
       "      <td>314.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>299.102719</td>\n",
       "      <td>314.805142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>274.212954</td>\n",
       "      <td>314.869790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>scg</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>275.697016</td>\n",
       "      <td>315.463098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>312.520360</td>\n",
       "      <td>315.525442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>311.107880</td>\n",
       "      <td>315.926237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>310.012628</td>\n",
       "      <td>316.517829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 10, 5, 5]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>280.448010</td>\n",
       "      <td>317.214707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>301.075594</td>\n",
       "      <td>317.228947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adam</td>\n",
       "      <td>[10, 5]</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>308.984131</td>\n",
       "      <td>317.699791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Method       Structure  Epochs  Learning Rate  Train RMSE   Test RMSE\n",
       "0     scg  [10, 10, 5, 5]    5000          0.010  269.088079  306.125934\n",
       "1    adam  [10, 10, 5, 5]    5000          0.005  288.897798  307.010033\n",
       "2    adam  [10, 10, 5, 5]    5000          0.001  293.722600  308.618803\n",
       "3     scg  [10, 10, 5, 5]    2000          0.005  286.009221  309.198389\n",
       "4     scg  [10, 10, 5, 5]    2000          0.010  296.775773  309.382869\n",
       "5    adam         [10, 5]    2000          0.010  308.784042  310.535040\n",
       "6    adam         [10, 5]    5000          0.010  305.417810  310.794719\n",
       "7     scg         [10, 5]    2000          0.005  304.065578  313.324705\n",
       "8    adam         [10, 5]   10000          0.005  297.802566  314.252859\n",
       "9    adam  [10, 10, 5, 5]    2000          0.010  290.348266  314.267240\n",
       "10    scg  [10, 10, 5, 5]    2000          0.001  287.849780  314.495050\n",
       "11    scg         [10, 5]    5000          0.010  299.102719  314.805142\n",
       "12   adam  [10, 10, 5, 5]   10000          0.010  274.212954  314.869790\n",
       "13    scg  [10, 10, 5, 5]    5000          0.001  275.697016  315.463098\n",
       "14   adam         [10, 5]    5000          0.001  312.520360  315.525442\n",
       "15   adam         [10, 5]    5000          0.005  311.107880  315.926237\n",
       "16   adam         [10, 5]    2000          0.005  310.012628  316.517829\n",
       "17   adam  [10, 10, 5, 5]    5000          0.010  280.448010  317.214707\n",
       "18   adam         [10, 5]   10000          0.010  301.075594  317.228947\n",
       "19   adam         [10, 5]   10000          0.001  308.984131  317.699791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df = pd.concat([sgd_results, adam_results, scg_results])\n",
    "performance_df = performance_df.sort_values(by='Test RMSE').reset_index(drop=True)\n",
    "performance_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several models with different optimizers, structures, epoch, and learning rate were fitted with the `Seoul Bike Hiring` dataset to find out the best model structure for this dataset.\n",
    "\n",
    "<h3> Performance of Optimizers </h3>\n",
    "\n",
    "The three optimizer tried in this experiment are-\n",
    "1. SDG\n",
    "2. Adam\n",
    "3. SCG\n",
    "\n",
    "<b> SCG </b> has the best performance on this dataset with the lowest <b> Test Rmse </b> of <b> 306.12 </b>. `Adam` also performs very well with a lowest <b> Test Rmse </b> of <b> 307.01 </b>\n",
    "\n",
    "<h3> Effect of Model Structure and Epoch </h3>\n",
    "\n",
    "Three model structures were tested during this experiment.\n",
    "\n",
    "1. More hidden layers: [10, 10, 5, 5]\n",
    "2. Less hidden layers: [10, 5]\n",
    "3. More units in hidden layers: [30, 10]\n",
    "\n",
    "For all the optimization methods, models with <b> More hidden layers </b> performs the best. The `Seould Bike` dataset has <b> Nine (09) </b> input variables which might be very correlated (can be better understood using a correlation plot). Neural Networks with more hidden layers are able to understand the non-linear relations between these variables well, hence the overall best performance of models with <b> More hidden layers </b>.\n",
    "\n",
    "With `Adam` and `SCG`, model achieves the best performance at epoch <b> 5000 </b>, making them faster optimizers than `SGD`. For `SGD`, model achieves the lowest rmse at `10000` epochs\n",
    "\n",
    "<h3> Overall Performance </h3>\n",
    "    \n",
    "From the experiments performed above with various neural network structure, the best model seems to be the model with <b> SCG</b> optimizer, <b>[10, 10, 5, 5]</b> structure, <b>5000 epoch</b>, and <b>0.010</b> learning rate. This model has a <b> Train RMSE</b> of <b>269.088</b> and <b>Test RMSE</b> of <b> 306.125</b>. The higher test rmse than train rmse means that the model is not overfitting with the training data. The output variable <b> Rented Bike </b> has a `min to max` range of `0 to 3556`. With a test rmse of `306.125`, the model will not perform very well for samples with lower values of `Rented Bike` but will perform better for samples with high value of the output variable. Model model structures, number of epochs, and learnig rates have to be tried to find a better model. Additional hyperparamters, such as activation functions can be tried to observe their effect. Moreover, complex and advanced models might also help to improve model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "Your notebook will be run and graded automatically. Test this grading process by first downloading [A3grader.tar](http://www.cs.colostate.edu/~anderson/cs545/notebooks/A3grader.tar) and extract `A3grader.py` from it. Run the code in the following cell to demonstrate an example grading session. As always, a different, but similar, grading script will be used to grade your checked-in notebook. It will include additional tests. You should design and perform additional tests on all of your functions to be sure they run correctly before checking in your notebook.  \n",
    "\n",
    "For the grading script to run correctly, you must first name this notebook as 'Lastname-A3.ipynb' with 'Lastname' being your last name, and then save this notebook. Check in your notebook in Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================= Code Execution =======================\n",
      "\n",
      "Extracting python code from notebook named 'A3 NeuralNetwork Class Using Optimizers.ipynb' and storing in notebookcode.py\n",
      "Removing all statements that are not function or class defs or import statements.\n",
      "\n",
      "Testing\n",
      "\n",
      "    import neuralnetworkA3 as nn\n",
      "\n",
      "    n_inputs = 3\n",
      "    n_hiddens = [5, 10, 20]\n",
      "    n_outputs = 2\n",
      "    n_samples = 10\n",
      "\n",
      "    X = np.arange(n_samples * n_inputs).reshape(n_samples, n_inputs) * 0.1\n",
      "    \n",
      "    nnet = nn.NeuralNetwork(n_inputs, n_hiddens, n_outputs)\n",
      "    nnet.all_weights[:] = 0.1  # set all weights to 0.1\n",
      "    nnet.X_means = np.mean(X, axis=0)\n",
      "    nnet.X_stds = np.std(X, axis=0)\n",
      "    nnet.T_means = np.zeros((n_samples, n_outputs))\n",
      "    nnet.T_stds = np.ones((n_samples, n_outputs))\n",
      "    \n",
      "    Y = nnet.use(X)\n",
      "\n",
      "\n",
      "--- 40/40 points. Returned correct value.\n",
      "\n",
      "Testing\n",
      "    n_inputs = 3\n",
      "    n_hiddens = [6, 3]\n",
      "    n_samples = 5\n",
      "\n",
      "    X = np.arange(n_samples * n_inputs).reshape(n_samples, n_inputs) * 0.1\n",
      "    T = np.log(X + 0.1)\n",
      "    n_outputs = T.shape[1]\n",
      "    \n",
      "    def rmse(A, B):\n",
      "        return np.sqrt(np.mean((A - B)**2))\n",
      "\n",
      "    results = []\n",
      "    for rep in range(20):\n",
      "        nnet = nn.NeuralNetwork(n_inputs, n_hiddens, n_outputs)\n",
      "        nnet.train(X, T, 5000, 'adam', 0.001, verbose=False)\n",
      "        Y = nnet.use(X)\n",
      "        err = rmse(Y, T)\n",
      "        print(f'Net {rep+1} RMSE {err:.5f}')\n",
      "        results.append(err)\n",
      "\n",
      "    mean_rmse = np.mean(results)\n",
      "    print(mean_rmse)\n",
      "\n",
      "Net 1 RMSE 0.00365\n",
      "Net 2 RMSE 0.00665\n",
      "Net 3 RMSE 0.00317\n",
      "Net 4 RMSE 0.00348\n",
      "Net 5 RMSE 0.00471\n",
      "Net 6 RMSE 0.00075\n",
      "Net 7 RMSE 0.00313\n",
      "Net 8 RMSE 0.00657\n",
      "Net 9 RMSE 0.00116\n",
      "Net 10 RMSE 0.00274\n",
      "Net 11 RMSE 0.00109\n",
      "Net 12 RMSE 0.00099\n",
      "Net 13 RMSE 0.00132\n",
      "Net 14 RMSE 0.00843\n",
      "Net 15 RMSE 0.00393\n",
      "Net 16 RMSE 0.00167\n",
      "Net 17 RMSE 0.00471\n",
      "Net 18 RMSE 0.00155\n",
      "Net 19 RMSE 0.00405\n",
      "Net 20 RMSE 0.00437\n",
      "0.003405818384769569\n",
      "\n",
      "--- 20/20 points. Returned correct value.\n",
      "\n",
      "Testing\n",
      "    n_inputs = 3\n",
      "    n_hiddens = []\n",
      "    n_samples = 5\n",
      "\n",
      "    X = np.arange(n_samples * n_inputs).reshape(n_samples, n_inputs) * 0.1\n",
      "    T = np.log(X + 0.1)\n",
      "    n_outputs = T.shape[1]\n",
      "    \n",
      "    def rmse(A, B):\n",
      "        return np.sqrt(np.mean((A - B)**2))\n",
      "\n",
      "    results = []\n",
      "    for rep in range(20):\n",
      "        nnet = nn.NeuralNetwork(n_inputs, n_hiddens, n_outputs)\n",
      "        nnet.train(X, T, 5000, 'adam', 0.001, verbose=False)\n",
      "        Y = nnet.use(X)\n",
      "        err = rmse(Y, T)\n",
      "        print(f'Net {rep+1} RMSE {err:.5f}')\n",
      "        results.append(err)\n",
      "\n",
      "    mean_rmse = np.mean(results)\n",
      "    print(mean_rmse)\n",
      "\n",
      "Net 1 RMSE 0.22315\n",
      "Net 2 RMSE 0.22315\n",
      "Net 3 RMSE 0.22315\n",
      "Net 4 RMSE 0.22315\n",
      "Net 5 RMSE 0.22315\n",
      "Net 6 RMSE 0.22315\n",
      "Net 7 RMSE 0.22315\n",
      "Net 8 RMSE 0.22315\n",
      "Net 9 RMSE 0.22315\n",
      "Net 10 RMSE 0.22315\n",
      "Net 11 RMSE 0.22315\n",
      "Net 12 RMSE 0.22315\n",
      "Net 13 RMSE 0.22315\n",
      "Net 14 RMSE 0.22315\n",
      "Net 15 RMSE 0.22315\n",
      "Net 16 RMSE 0.22315\n",
      "Net 17 RMSE 0.22315\n",
      "Net 18 RMSE 0.22315\n",
      "Net 19 RMSE 0.22315\n",
      "Net 20 RMSE 0.22315\n",
      "0.22315402344078722\n",
      "\n",
      "--- 20/20 points. Returned correct value.\n",
      "\n",
      "======================================================================\n",
      "F:\\PhD_Course_materials_documents\\Course_Materials_Fall_2022\\CS_545_Machine_Learning\\Assignments\\Assignment_3 Execution Grade is 80 / 80\n",
      "======================================================================\n",
      "\n",
      "___ / 5 Correctly read in bike sharing data using pandas.read_csv.\n",
      "___ / 5 Correctly ran the required experiments.\n",
      "___ / 10 Correctly show resulting pandas DataFrame and discuss results.\n",
      "\n",
      "======================================================================\n",
      "F:\\PhD_Course_materials_documents\\Course_Materials_Fall_2022\\CS_545_Machine_Learning\\Assignments\\Assignment_3 Experiments and Discussion Grade is __ / 20\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "F:\\PhD_Course_materials_documents\\Course_Materials_Fall_2022\\CS_545_Machine_Learning\\Assignments\\Assignment_3 FINAL GRADE is  _  / 100\n",
      "======================================================================\n",
      "\n",
      "Extra Credit:  Code and discussion showing most significant input features.\n",
      "\n",
      "\n",
      "F:\\PhD_Course_materials_documents\\Course_Materials_Fall_2022\\CS_545_Machine_Learning\\Assignments\\Assignment_3 EXTRA CREDIT is 0 / 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i A3grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "Using a network that gives you pretty good test RMSE results, try to figure out which input features are most significant in predicting the bike-share count.  Remember, that our neural networks is trained with standardized inputs, so you can compare the magnitudes of weights in the first layer to help you determine which inputs are most significant. \n",
    "\n",
    "To visualize the weights, try displaying the weights in the first layer as an image, with `plt.imshow` with `plt.colorbar()`. Discuss which weights have the largest magnitudes and discuss any patterns in see in the weights in each hidden unit of the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2> Finding Out the Most Significant Predictor </h2>\n",
    " \n",
    " We will perform this analysis with the best model from previous experiment which is trained with <b> Adam </b> optimizer, <b> [10, 10, 5, 5] </b> structure, <b> 5000 </b> epoch, and <b> 0.001 </b> learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Epoch 500 RMSE=0.60503\n",
      "Adam: Epoch 1000 RMSE=0.53342\n",
      "Adam: Epoch 1500 RMSE=0.50923\n",
      "Adam: Epoch 2000 RMSE=0.49340\n",
      "Adam: Epoch 2500 RMSE=0.48056\n",
      "Adam: Epoch 3000 RMSE=0.47227\n",
      "Adam: Epoch 3500 RMSE=0.46330\n",
      "Adam: Epoch 4000 RMSE=0.45682\n",
      "Adam: Epoch 4500 RMSE=0.45237\n",
      "Adam: Epoch 5000 RMSE=0.44945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(9, [10, 10, 5, 5], 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('SeoulBikeData.csv')\n",
    "T = data['Rented Bike Count'].to_numpy().reshape(-1, 1)\n",
    "X = data[['Hour', 'Temperature(C)', 'Humidity(%)',\n",
    "          'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(C)',\n",
    "          'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']].to_numpy()\n",
    "\n",
    "\n",
    "Xtrain, Ttrain, Xtest, Ttest = partition(X, T, 0.8)\n",
    "n_inputs, n_outputs = Xtrain.shape[1], Ttrain.shape[1] \n",
    "\n",
    "best_model = nn.NeuralNetwork(n_inputs, [10, 10, 5, 5], n_outputs)\n",
    "best_model.train(Xtrain, Ttrain, n_epochs=5000, method='adam', learning_rate=0.001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.Ws[0].shape # shape of first weight. This includes bias weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x2a3d88864c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "cmap = LinearSegmentedColormap.from_list('rg',[\"r\", \"w\", \"g\"], N=256) \n",
    "\n",
    "columns = ['bias', 'Hour', 'Temperature(C)', 'Humidity(%)',\n",
    "          'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(C)',\n",
    "          'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\n",
    "\n",
    "W0 = best_model.Ws[0]\n",
    "n_units = W0.shape[1]\n",
    "\n",
    "mx = np.max(np.abs(W0))\n",
    "\n",
    "plt.imshow(W0, vmin=-mx, vmax=mx, cmap=cmap)\n",
    "xlabels = ['unit{}'.format(y) for y in range(n_units)]\n",
    "plt.xticks(range(n_units), xlabels, rotation=45)\n",
    "plt.yticks(range(len(columns)), columns)\n",
    "plt.title('Weights of the first layer in the NN')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `Weights of the first layer in the NN` plot, it is visible that variables `Hour, Temperature(C), Humidity(%), Rainfall(mm), Snowfall(cm)` are some of the most significant positive/negative weights assigned in various hidden layers. Therefore, in this model, these variables can be considered as the most important predictors. \n",
    "\n",
    "Both positive and negative weights have been assigned in the abovementioned variables. For example, `Rainfall` has positive weight assigned in unit 8 and negative in unit 6. This may signify opposite correlation to rainfall event and bike hiring. When rainfall is high bike sharing reduces and vice versa. Similar trends can be seen in weights of `Snowfall`. Bike hiring seems to be strongly correlated to `Hour`. Positive weights in `Hour` might indicate peak hours in which bike hiring is high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
